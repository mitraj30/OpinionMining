{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "completed.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-nV_l8CmJ74",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://rhyme.com/assets/img/logo-dark.png\" align=\"center\"> <h2 align=\"center\">Logistic Regression: A Sentiment Analysis Case Study</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO6B9yRumJ8X",
        "colab_type": "text"
      },
      "source": [
        "### Introduction\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEmtRCHomJ8g",
        "colab_type": "text"
      },
      "source": [
        "- IMDB movie reviews dataset\n",
        "- http://ai.stanford.edu/~amaas/data/sentiment\n",
        "- Contains 25000 positive and 25000 negative reviews\n",
        "<img src=\"https://i.imgur.com/lQNnqgi.png\" align=\"center\">\n",
        "- Contains at most reviews per movie\n",
        "- At least 7 stars out of 10 $\\rightarrow$ positive (label = 1)\n",
        "- At most 4 stars out of 10 $\\rightarrow$ negative (label = 0)\n",
        "- 50/50 train/test split\n",
        "- Evaluation accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFjPlD91mJ8n",
        "colab_type": "text"
      },
      "source": [
        "<b>Features: bag of 1-grams with TF-IDF values</b>:\n",
        "- Extremely sparse feature matrix - close to 97% are zeros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZHQRQ2vmJ8v",
        "colab_type": "text"
      },
      "source": [
        " <b>Model: Logistic regression</b>\n",
        "- $p(y = 1|x) = \\sigma(w^{T}x)$\n",
        "- Linear classification model\n",
        "- Can handle sparse data\n",
        "- Fast to train\n",
        "- Weights can be interpreted\n",
        "<img src=\"https://i.imgur.com/VieM41f.png\" align=\"center\" width=500 height=500>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnc6Zj4imJ9J",
        "colab_type": "text"
      },
      "source": [
        "### Task 1: Loading the dataset\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJvmCA0rmJ9N",
        "colab_type": "code",
        "colab": {},
        "outputId": "109f38ea-5914-4914-8d37-193c19c20a23"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('./movie_data.csv')\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hi for all the people who have seen this wonde...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I recently bought the DVD, forgetting just how...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
              "1  OK... so... I really like Kris Kristofferson a...          0\n",
              "2  ***SPOILER*** Do not read this, if you think a...          0\n",
              "3  hi for all the people who have seen this wonde...          1\n",
              "4  I recently bought the DVD, forgetting just how...          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmFnICobmJ9j",
        "colab_type": "text"
      },
      "source": [
        "## <h2 align=\"center\">Bag of words / Bag of N-grams model</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebXGNDgcmJ9n",
        "colab_type": "text"
      },
      "source": [
        "### Task 2: Transforming documents into feature vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6Oqpd5SmJ9v",
        "colab_type": "text"
      },
      "source": [
        "Below, we will call the fit_transform method on CountVectorizer. This will construct the vocabulary of the bag-of-words model and transform the following three sentences into sparse feature vectors:\n",
        "1. The sun is shining\n",
        "2. The weather is sweet\n",
        "3. The sun is shining, the weather is sweet, and one and one is two\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOlXQxCdmJ9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count = CountVectorizer()\n",
        "docs = np.array([\n",
        "        'The sun is shining',\n",
        "        'The weather is sweet',\n",
        "        'The sun is shining, the weather is sweet, and one and one is two'])\n",
        "bag = count.fit_transform(docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxEaa8b7mJ-C",
        "colab_type": "code",
        "colab": {},
        "outputId": "31d9b9f8-f5e8-4578-95c8-d4e1a60cbac9"
      },
      "source": [
        "print(count.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw0tbT_MmJ-S",
        "colab_type": "code",
        "colab": {},
        "outputId": "77053e0d-d4a4-4660-9956-0755514e3ea9"
      },
      "source": [
        "print(bag.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 0 1 1 0 1 0 0]\n",
            " [0 1 0 0 0 1 1 0 1]\n",
            " [2 3 2 1 1 1 2 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgMEcK3RmJ-l",
        "colab_type": "text"
      },
      "source": [
        "Raw term frequencies: *tf (t,d)*â€”the number of times a term t occurs in a document *d*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHFiqmAGmJ-n",
        "colab_type": "text"
      },
      "source": [
        "### Task 3: Word relevancy using term frequency-inverse document frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uMgkHr1mJ-t",
        "colab_type": "text"
      },
      "source": [
        "$$\\text{tf-idf}(t,d)=\\text{tf (t,d)}\\times \\text{idf}(t,d)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67GMr47KmJ-v",
        "colab_type": "text"
      },
      "source": [
        "$$\\text{idf}(t,d) = \\text{log}\\frac{n_d}{1+\\text{df}(d, t)},$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_M84dSHmJ-y",
        "colab_type": "text"
      },
      "source": [
        "where $n_d$ is the total number of documents, and df(d, t) is the number of documents d that contain the term t."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh_W3dOdmJ-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.set_printoptions(precision=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W42LaBgDmJ_I",
        "colab_type": "code",
        "colab": {},
        "outputId": "a9d0d079-004f-47aa-f264-cd9767b46f38"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
        "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
            " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
            " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsntdkPXmJ_Z",
        "colab_type": "text"
      },
      "source": [
        "The equations for the idf and tf-idf that are implemented in scikit-learn are:\n",
        "\n",
        "$$\\text{idf} (t,d) = log\\frac{1 + n_d}{1 + \\text{df}(d, t)}$$\n",
        "The tf-idf equation that is implemented in scikit-learn is as follows:\n",
        "\n",
        "$$\\text{tf-idf}(t,d) = \\text{tf}(t,d) \\times (\\text{idf}(t,d)+1)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebGcEwTmmJ_c",
        "colab_type": "text"
      },
      "source": [
        "$$v_{\\text{norm}} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v_{1}^{2} + v_{2}^{2} + \\dots + v_{n}^{2}}} = \\frac{v}{\\big (\\sum_{i=1}^{n} v_{i}^{2}\\big)^\\frac{1}{2}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXZlfIvsmJ_g",
        "colab_type": "text"
      },
      "source": [
        "### Example:\n",
        "$$\\text{idf}(\"is\", d3) = log \\frac{1+3}{1+3} = 0$$\n",
        "Now in order to calculate the tf-idf, we simply need to add 1 to the inverse document frequency and multiply it by the term frequency:\n",
        "\n",
        "$$\\text{tf-idf}(\"is\",d3)= 3 \\times (0+1) = 3$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hITosGzMmJ_n",
        "colab_type": "text"
      },
      "source": [
        "### Task 4: Calculate tf-idf of the term *is*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkk6Tw_-mJ_s",
        "colab_type": "code",
        "colab": {},
        "outputId": "ac6799ff-f512-45e3-91d8-a67fb317674f"
      },
      "source": [
        "tf_is = 3\n",
        "n_docs = 3\n",
        "idf_is = np.log((n_docs+1) / (3+1))\n",
        "tfidf_is = tf_is * (idf_is + 1)\n",
        "print('tf-idf of term \"is\" = %.2f' % tfidf_is)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf-idf of term \"is\" = 3.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PduXrP3XmKAI",
        "colab_type": "text"
      },
      "source": [
        "$$\\text{tfi-df}_{norm} = \\frac{[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]}{\\sqrt{[3.39^2, 3.0^2, 3.39^2, 1.29^2, 1.29^2, 1.29^2, 2.0^2 , 1.69^2, 1.29^2]}}$$$$=[0.5, 0.45, 0.5, 0.19, 0.19, 0.19, 0.3, 0.25, 0.19]$$$$\\Rightarrow \\text{tfi-df}_{norm}(\"is\", d3) = 0.45$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmAkY-P6mKAL",
        "colab_type": "code",
        "colab": {},
        "outputId": "ba8c6914-d8ad-48d8-a199-fa04a8460c04"
      },
      "source": [
        "tfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True)\n",
        "raw_tfidf = tfidf.fit_transform(count.fit_transform(docs)).toarray()[-1]\n",
        "raw_tfidf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3.39, 3.  , 3.39, 1.29, 1.29, 1.29, 2.  , 1.69, 1.29])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXC2mYilmKAX",
        "colab_type": "code",
        "colab": {},
        "outputId": "0476864d-d6e1-4c25-cd50-bd720d3390b7"
      },
      "source": [
        "l2_tfidf = raw_tfidf / np.sqrt(np.sum(raw_tfidf**2))\n",
        "l2_tfidf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.5 , 0.45, 0.5 , 0.19, 0.19, 0.19, 0.3 , 0.25, 0.19])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwLVzLctmKAd",
        "colab_type": "text"
      },
      "source": [
        "### Task 5:Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3Y6s9qImKAk",
        "colab_type": "code",
        "colab": {},
        "outputId": "a1eeb84b-237c-4296-81fe-1bcca4aef1d7"
      },
      "source": [
        "df.loc[0, 'review'][-50:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'is seven.<br /><br />Title (Brazil): Not Available'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmjlskaDmKAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def preprocessor(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
        "        ' '.join(emoticons).replace('-', '')\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YepKd8EGmKA0",
        "colab_type": "code",
        "colab": {},
        "outputId": "632e8a1f-7c5d-4774-8789-1d03d6e6441f"
      },
      "source": [
        "preprocessor(df.loc[0, 'review'][-50:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'is seven title brazil not available'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss1JcfNYmKA-",
        "colab_type": "code",
        "colab": {},
        "outputId": "c86be2be-429c-4809-bf2d-91e3cc9598e7"
      },
      "source": [
        "preprocessor(\"</a>This :) is :( a test :-)!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this is a test :) :( :)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjYzNrWUmKBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['review'] = df['review'].apply(preprocessor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBJQfteMmKBO",
        "colab_type": "text"
      },
      "source": [
        "### Task 6: Tokenization of documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWHiELs6mKBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "\n",
        "def tokenizer(text):\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "def tokenizer_porter(text):\n",
        "    return [porter.stem(word) for word in text.split()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynmjvzIDmKBY",
        "colab_type": "code",
        "colab": {},
        "outputId": "028db7cc-ebfb-4b15-e70d-847397e532d1"
      },
      "source": [
        "tokenizer('runners like running and thus they run')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZINox5PJmKBd",
        "colab_type": "code",
        "colab": {},
        "outputId": "8f5e8b8c-e30c-42d7-90c9-7ec467880319"
      },
      "source": [
        "tokenizer_porter('runners like running and thus they run')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp7bpKJQmKBm",
        "colab_type": "code",
        "colab": {},
        "outputId": "3d6c9a7f-a995-4919-a864-00fd6efc8490"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSJRsp1omKBs",
        "colab_type": "code",
        "colab": {},
        "outputId": "e11f813b-cac4-4c96-8a3d-486ecf249d22"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "[w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:]\n",
        "if w not in stop]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['runner', 'like', 'run', 'run', 'lot']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sh4w05RmKB4",
        "colab_type": "text"
      },
      "source": [
        "### Task 7: Document classification via a logistic regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zJ-zpgfmKB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = df.loc[:25000, 'review'].values\n",
        "y_train = df.loc[:25000, 'sentiment'].values\n",
        "X_test = df.loc[25000:, 'review'].values\n",
        "y_test = df.loc[25000:, 'sentiment'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmCVaHfCmKB_",
        "colab_type": "code",
        "colab": {},
        "outputId": "aebca17c-5078-4151-ad04-572ab3a3225e"
      },
      "source": [
        "print(\"Review: \", X_train[1:2], \"\\n\")\n",
        "print(\"Sentiment: \", y_train[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review:  ['ok so i really like kris kristofferson and his usual easy going delivery of lines in his movies age has helped him with his soft spoken low energy style and he will steal a scene effortlessly but disappearance is his misstep holy moly this was a bad movie i must give kudos to the cinematography and and the actors including kris for trying their darndest to make sense from this goofy confusing story none of it made sense and kris probably didn t understand it either and he was just going through the motions hoping someone would come up to him and tell him what it was all about i don t care that everyone on this movie was doing out of love for the project or some such nonsense i ve seen low budget movies that had a plot for goodness sake this had none zilcho nada zippo empty of reason a complete waste of good talent scenery and celluloid i rented this piece of garbage for a buck and i want my money back i want my 2 hours back i invested on this grade f waste of my time don t watch this movie or waste 1 minute of your valuable time while passing through a room where it s playing or even open up the case that is holding the dvd believe me you ll thank me for the advice '] \n",
            "\n",
            "Sentiment:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiGWqNzxmKCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "tfidf = TfidfVectorizer(strip_accents=None,\n",
        "                        lowercase=False,\n",
        "                        preprocessor=None)\n",
        "\n",
        "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
        "               'vect__stop_words': [stop, None],\n",
        "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
        "               'clf__penalty': ['l2'],\n",
        "               'clf__C': [1.0, 10.0, 100.0]},\n",
        "              {'vect__ngram_range': [(1, 1)],\n",
        "               'vect__stop_words': [stop, None],\n",
        "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
        "               'vect__use_idf':[False],\n",
        "               'vect__norm':[None],\n",
        "               'clf__penalty': ['l2'],\n",
        "               'clf__C': [1.0, 10.0, 100.0]},\n",
        "              ]\n",
        "\n",
        "lr_tfidf = Pipeline([('vect', tfidf),\n",
        "                     ('clf', LogisticRegression(random_state=0))])\n",
        "\n",
        "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
        "                           scoring='accuracy',\n",
        "                           cv=5,\n",
        "                           verbose=1,\n",
        "                           n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGtlGBWomKCK",
        "colab_type": "text"
      },
      "source": [
        "### Task 8: Load saved model from disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUKoyjZ_mKCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vyftOylmKCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'saved_model.sav'\n",
        "gs_lr_tfidf = pickle.load(open(filename, 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uARqxdGImKCX",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cdX3dKZmKCZ",
        "colab_type": "text"
      },
      "source": [
        "### Task 9: Model accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0MOQJvamKCa",
        "colab_type": "code",
        "colab": {},
        "outputId": "7fdaeba0-8319-4d9b-8e00-65a93831ff1a"
      },
      "source": [
        "print('Best parameter set: %s' % gs_lr_tfidf.best_params_)\n",
        "print('CV Accuracy:%.3f' % gs_lr_tfidf.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x000001544E5C0488>}\n",
            "CV Accuracy:0.897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLn2vT0BmKCf",
        "colab_type": "code",
        "colab": {},
        "outputId": "0f8c7c4f-fd8a-4a79-d48e-b3fff67afd16"
      },
      "source": [
        "clf = gs_lr_tfidf.best_estimator_\n",
        "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.899\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}